{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../src/models/')\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "# import pandas as pd\n",
    "\n",
    "from src.code_snippets.dataprep.embeddings_preprocessing.data_preparation import (\n",
    "    sentences_to_indices,\n",
    "    pretrained_embedding_layer\n",
    ")\n",
    "\n",
    "from src.code_snippets.utils.abstract_classes import Trainer\n",
    "from src.code_snippets.dataprep.embeddings_preprocessing.glove.reader import read_glove_file,get_word_index_dicts\n",
    "from src.code_snippets.dataprep.embeddings_preprocessing.data_preparation import pretrained_embedding_layer\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ManyToOneSeqModel(Trainer):\n",
    "\n",
    "    def __init__(self,train_data,val_data,embedding_dir):\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.gensim_model = read_glove_file(self.embedding_dir)\n",
    "        self.word_to_index,self.index_to_words = get_word_index_dicts(self.gensim_model)\n",
    "        \n",
    "        self.m_X, self.n_X = self.train_data['X_indices_train'].shape\n",
    "        self.m_X_aux, self.n_X_aux = self.train_data['X_aux_train'].shape\n",
    "    \n",
    "    def set_model(self):\n",
    "        pretrained_embedding_layer(self.gensim_model, self.word_to_index)\n",
    "    \n",
    "    def save_model(self):\n",
    "        pass\n",
    "    def fit_model(self):\n",
    "        pass\n",
    "    def generate_metrics(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,LSTM,Conv1D,Dropout, Dense\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "              'X_aux_train': np.load('../../data/processed/X_aux_train.npy'),\n",
    "              'X_indices_train':np.load('../../data/processed/X_indices_train.npy').astype('int32'),\n",
    "              'y_train' : np.load('../../data/processed/y_train.npy')\n",
    "             }\n",
    "\n",
    "val_data = {\n",
    "            'X_aux_val': np.load('../../data/processed/X_aux_val.npy'),\n",
    "            'X_indices_val': np.load('../../data/processed/X_indices_val.npy').astype('int32'),\n",
    "            'y_val' : np.load('../../data/processed/y_val.npy')\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531\n"
     ]
    }
   ],
   "source": [
    "num_observations = len(train_data['X_aux_train'])\n",
    "idx = np.arange(num_observations)\n",
    "\n",
    "sample_portion = 0.01\n",
    "sample_size = int(num_observations*sample_portion)\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "sampled_idx = np.random.choice(idx,sample_size,replace = False)\n",
    "\n",
    "train_data['X_indices_train'] = train_data['X_indices_train'][sampled_idx]\n",
    "train_data['X_aux_train'] = train_data['X_aux_train'][sampled_idx]\n",
    "train_data['y_train'] = train_data['y_train'][sampled_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ManyToOneSeqModel(train_data,\n",
    "                            val_data,\n",
    "                            \"../../../../pretrained_embeddings/glove.twitter.27B/glove.twitter.27B.25d.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_indices = Input((trainer.n_X),dtype='int32')\n",
    "\n",
    "# Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n",
    "embedding_layer = pretrained_embedding_layer(trainer.gensim_model,\n",
    "                                             trainer.word_to_index)\n",
    "\n",
    "# Propagate sentence_indices through your embedding layer\n",
    "# (See additional hints in the instructions).\n",
    "embeddings = embedding_layer(sentence_indices)   \n",
    "\n",
    "X = LSTM(128,return_sequences=True)(embeddings)\n",
    "\n",
    "X = Dropout(0.1,seed= seed)(X)\n",
    "\n",
    "X = LSTM(128,return_sequences=False)(X)\n",
    "\n",
    "X = Dropout(0.1,seed= seed)(X)\n",
    "\n",
    "X = Dense(16,activation='relu')(X)\n",
    "\n",
    "X = Dropout(0.1,seed= seed)(X)\n",
    "\n",
    "X = Dense(1,activation='sigmoid')(X)\n",
    "\n",
    "\n",
    "model = Model(sentence_indices,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 3000, 25)          29837875  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 3000, 128)         78848     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3000, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 30,050,388\n",
      "Trainable params: 212,513\n",
      "Non-trainable params: 29,837,875\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',metrics = [f1_metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1531 samples\n",
      "Epoch 1/50\n",
      "1531/1531 [==============================] - 266s 174ms/sample - loss: 0.3504 - f1_metric: 0.0372\n",
      "Epoch 2/50\n",
      "1531/1531 [==============================] - 276s 181ms/sample - loss: 0.2870 - f1_metric: 0.2918\n",
      "Epoch 3/50\n",
      "1531/1531 [==============================] - 262s 171ms/sample - loss: 0.2603 - f1_metric: 0.4333\n",
      "Epoch 4/50\n",
      "1531/1531 [==============================] - 286s 187ms/sample - loss: 0.2319 - f1_metric: 0.4627\n",
      "Epoch 5/50\n",
      "1531/1531 [==============================] - 324s 212ms/sample - loss: 0.2250 - f1_metric: 0.4811\n",
      "Epoch 6/50\n",
      "1531/1531 [==============================] - 345s 225ms/sample - loss: 0.2103 - f1_metric: 0.5469\n",
      "Epoch 7/50\n",
      "1531/1531 [==============================] - 362s 237ms/sample - loss: 0.2050 - f1_metric: 0.5221\n",
      "Epoch 8/50\n",
      "1531/1531 [==============================] - 354s 232ms/sample - loss: 0.1884 - f1_metric: 0.6368\n",
      "Epoch 9/50\n",
      "1531/1531 [==============================] - 289s 189ms/sample - loss: 0.1758 - f1_metric: 0.5916\n",
      "Epoch 10/50\n",
      "1531/1531 [==============================] - 270s 176ms/sample - loss: 0.1642 - f1_metric: 0.6636\n",
      "Epoch 11/50\n",
      "1531/1531 [==============================] - 1115s 728ms/sample - loss: 0.1539 - f1_metric: 0.6147\n",
      "Epoch 12/50\n",
      "1531/1531 [==============================] - 456s 298ms/sample - loss: 0.1461 - f1_metric: 0.6964\n",
      "Epoch 13/50\n",
      "1531/1531 [==============================] - 265s 173ms/sample - loss: 0.1365 - f1_metric: 0.7330\n",
      "Epoch 14/50\n",
      "1531/1531 [==============================] - 2206s 1s/sample - loss: 0.1261 - f1_metric: 0.7469\n",
      "Epoch 15/50\n",
      "1531/1531 [==============================] - 267s 175ms/sample - loss: 0.1107 - f1_metric: 0.7939\n",
      "Epoch 16/50\n",
      "1531/1531 [==============================] - 269s 176ms/sample - loss: 0.1100 - f1_metric: 0.8265\n",
      "Epoch 17/50\n",
      "1531/1531 [==============================] - 285s 186ms/sample - loss: 0.0994 - f1_metric: 0.8034\n",
      "Epoch 18/50\n",
      "1531/1531 [==============================] - 289s 189ms/sample - loss: 0.0782 - f1_metric: 0.8722\n",
      "Epoch 19/50\n",
      "1531/1531 [==============================] - 294s 192ms/sample - loss: 0.1007 - f1_metric: 0.8668\n",
      "Epoch 20/50\n",
      "1531/1531 [==============================] - 297s 194ms/sample - loss: 0.0837 - f1_metric: 0.8414\n",
      "Epoch 21/50\n",
      "1531/1531 [==============================] - 295s 192ms/sample - loss: 0.0566 - f1_metric: 0.8646\n",
      "Epoch 22/50\n",
      "1531/1531 [==============================] - 306s 200ms/sample - loss: 0.0607 - f1_metric: 0.8313\n",
      "Epoch 23/50\n",
      "1531/1531 [==============================] - 307s 200ms/sample - loss: 0.0594 - f1_metric: 0.8866\n",
      "Epoch 24/50\n",
      "1531/1531 [==============================] - 335s 219ms/sample - loss: 0.0423 - f1_metric: 0.9010\n",
      "Epoch 25/50\n",
      "1531/1531 [==============================] - 321s 210ms/sample - loss: 0.0355 - f1_metric: 0.8903\n",
      "Epoch 26/50\n",
      "1531/1531 [==============================] - 316s 207ms/sample - loss: 0.0310 - f1_metric: 0.9480\n",
      "Epoch 27/50\n",
      "1531/1531 [==============================] - 312s 203ms/sample - loss: 0.0391 - f1_metric: 0.9215\n",
      "Epoch 28/50\n",
      "1531/1531 [==============================] - 300s 196ms/sample - loss: 0.0283 - f1_metric: 0.9479\n",
      "Epoch 29/50\n",
      "1531/1531 [==============================] - 303s 198ms/sample - loss: 0.0229 - f1_metric: 0.9582\n",
      "Epoch 30/50\n",
      "1531/1531 [==============================] - 295s 193ms/sample - loss: 0.0368 - f1_metric: 0.9513\n",
      "Epoch 31/50\n",
      "1531/1531 [==============================] - 279s 182ms/sample - loss: 0.0063 - f1_metric: 0.9564\n",
      "Epoch 32/50\n",
      "1531/1531 [==============================] - 276s 180ms/sample - loss: 0.0193 - f1_metric: 0.9200\n",
      "Epoch 33/50\n",
      "1531/1531 [==============================] - 277s 181ms/sample - loss: 0.0181 - f1_metric: 0.9298\n",
      "Epoch 34/50\n",
      "1376/1531 [=========================>....] - ETA: 29s - loss: 0.0087 - f1_metric: 0.9683"
     ]
    }
   ],
   "source": [
    "model.fit(trainer.train_data['X_indices_train'],\n",
    "          trainer.train_data['y_train'],\n",
    "          epochs=50,\n",
    "          batch_size=2**5,\n",
    "          shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda15aae41925a8475e86b7bea0cce036d9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
